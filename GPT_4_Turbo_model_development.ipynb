{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For GPT-4 Turbo\n",
    "\n",
    "Use your own API Key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "import sacrebleu\n",
    "from rouge_score import rouge_scorer\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############ 1. Data Preparation for Model Training by Combine the datasets and prepare them for training ############\n",
    "# Load cleaned Counsel-Chat dataset\n",
    "counsel_chat_data = pd.read_csv('/Users/dipendrapant/Library/CloudStorage/OneDrive-NTNU/ForFun/npj_digital_medicine/TechnicalEvaluationGPT4/data/counsel_chat_data_after_data_preparation.csv')\n",
    "display(counsel_chat_data.info())\n",
    "display(counsel_chat_data.head(4))\n",
    "# Rename columns for consistency with previous combined data format\n",
    "counsel_chat_data.rename(\n",
    "    columns={'questionText': 'prompt', 'answerText': 'response'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ 2. Generate Responses Using GPT-4 Model via OpenAI API ############\n",
    "\n",
    "openai.api_key = 'sk-proj-OpenAI_API_Key'\n",
    "\n",
    "\n",
    "def generate_responses(data, model_name='gpt-4-turbo'):\n",
    "    responses = []\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        prompt = row['prompt']\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=model_name,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a mental health counselor. Your goal is to provide empathetic, supportive, and reflective responses to clients' questions. Focus on understanding the clients' concerns and offering thoughtful and compassionate guidance.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                max_tokens=150,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            responses.append(response.choices[0].message['content'].strip())\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating response for index {index}: {e}\")\n",
    "            responses.append(\"\")\n",
    "\n",
    "    data['generated_response'] = responses\n",
    "    data.to_csv('/Users/dipendrapant/Library/CloudStorage/OneDrive-NTNU/ForFun/npj_digital_medicine/code/data/result/v3/generated_responses.csv', index=False)\n",
    "    return data\n",
    "\n",
    "\n",
    "# Generate responses for the dataset\n",
    "counsel_chat_data = generate_responses(counsel_chat_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ 2. Evaluation of Generated Responses ############\n",
    "\n",
    "def evaluate_responses(data):\n",
    "    bleu_scores = []\n",
    "    rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        reference = row['response']\n",
    "        hypothesis = row['generated_response']\n",
    "\n",
    "        try:\n",
    "            # BLEU Score\n",
    "            bleu_score = sacrebleu.sentence_bleu(\n",
    "                hypothesis, [reference]).score / 100  # Normalize BLEU score\n",
    "            bleu_scores.append(bleu_score)\n",
    "\n",
    "            # ROUGE Score\n",
    "            scorer = rouge_scorer.RougeScorer(\n",
    "                ['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "            scores = scorer.score(reference, hypothesis)\n",
    "            for key in scores:\n",
    "                rouge_scores[key].append(scores[key].fmeasure)\n",
    "\n",
    "            print(f\"Processed index {index}: BLEU = {bleu_score}, ROUGE-1 = {scores['rouge1'].fmeasure}, ROUGE-2 = {\n",
    "                  scores['rouge2'].fmeasure}, ROUGE-L = {scores['rougeL'].fmeasure}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing index {index}: {e}\")\n",
    "\n",
    "    evaluation_results = {\n",
    "        'bleu': sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0,\n",
    "        'rouge1': sum(rouge_scores['rouge1']) / len(rouge_scores['rouge1']) if rouge_scores['rouge1'] else 0,\n",
    "        'rouge2': sum(rouge_scores['rouge2']) / len(rouge_scores['rouge2']) if rouge_scores['rouge2'] else 0,\n",
    "        'rougeL': sum(rouge_scores['rougeL']) / len(rouge_scores['rougeL']) if rouge_scores['rougeL'] else 0\n",
    "    }\n",
    "\n",
    "    return evaluation_results\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "counsel_chat_data = pd.read_csv(\n",
    "    '/Users/dipendrapant/Library/CloudStorage/OneDrive-NTNU/ForFun/npj_digital_medicine/code/data/result/v3/generated_responses.csv')\n",
    "evaluation_results = evaluate_responses(counsel_chat_data)\n",
    "\n",
    "print(\"Evaluation Results:\", evaluation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ 3. Analyze BLEU Score Distribution ############\n",
    "def calculate_bleu_scores(data):\n",
    "    bleu_scores = []\n",
    "    try:\n",
    "        print(f\"Data loaded successfully. Total records: {len(data)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        reference = row['response']\n",
    "        hypothesis = row['generated_response']\n",
    "        bleu_score = sacrebleu.sentence_bleu(hypothesis, [reference]).score\n",
    "        bleu_scores.append(bleu_score)\n",
    "\n",
    "    data['bleu_score'] = bleu_scores\n",
    "    return data\n",
    "\n",
    "\n",
    "counsel_chat_data = pd.read_csv(\n",
    "    '/Users/dipendrapant/Library/CloudStorage/OneDrive-NTNU/ForFun/npj_digital_medicine/TechnicalEvaluationGPT4/data/result/v3/generated_responses.csv')\n",
    "evaluated_data = calculate_bleu_scores(counsel_chat_data)\n",
    "\n",
    "# Plot Distribution:\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(evaluated_data['bleu_score'], bins=50,\n",
    "         color='skyblue', edgecolor='black', alpha=0.5)\n",
    "#plt.title('Distribution of BLEU Scores')\n",
    "plt.xlabel('BLEU Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig('/Users/dipendrapant/Library/CloudStorage/OneDrive-NTNU/ForFun/npj_digital_medicine/TechnicalEvaluationGPT4/data/result/v3/gpt_4_turbo_bleu_score_distribution.png', dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ 4. Length Analysis of Generated Responses ############\n",
    "print(evaluated_data.info())\n",
    "evaluated_data['reference_length'] = evaluated_data['response'].apply(len)\n",
    "evaluated_data['generated_length'] = evaluated_data['generated_response'].apply(\n",
    "    len)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(evaluated_data.index,\n",
    "            evaluated_data['reference_length'], color='blue', label='Reference Response Length')\n",
    "plt.scatter(evaluated_data.index,\n",
    "            evaluated_data['generated_length'], color='orange', label='Generated Response Length')\n",
    "\n",
    "plt.xlabel('Data Index')\n",
    "plt.ylabel('Response Length')\n",
    "#plt.title('Length Comparison of Reference and Generated Responses')\n",
    "plt.legend()\n",
    "#plt.savefig('/Users/dipendrapant/Library/CloudStorage/OneDrive-NTNU/ForFun/npj_digital_medicine/TechnicalEvaluationGPT4/data/result/v3/gpt_4_turbo_length_analysis.eps', format='eps', dpi=600)\n",
    "plt.show()\n",
    "\n",
    "############ 5. Content Analysis of Generated Responses ############\n",
    "\n",
    "generated_responses = evaluated_data['generated_response'].tolist()\n",
    "word_counts = Counter(\" \".join(generated_responses).split())\n",
    "\n",
    "common_words = word_counts.most_common(40)\n",
    "print(\"Most common words in generated responses:\")\n",
    "for word, count in common_words:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "############ 6. Sentiment Analysis and Emotion Detection ############\n",
    "\n",
    "# Function to perform sentiment analysis\n",
    "\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "# Function to perform emotion detection\n",
    "\n",
    "\n",
    "def detect_emotions(text):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    return sid.polarity_scores(text)\n",
    "\n",
    "# Adding sentiment analysis and emotion detection to the evaluation\n",
    "\n",
    "\n",
    "def evaluate_responses_with_sentiment(data):\n",
    "    # Perform sentiment analysis\n",
    "    data['sentiment'] = data['generated_response'].apply(\n",
    "        lambda x: analyze_sentiment(x))\n",
    "\n",
    "    # Perform emotion detection\n",
    "    emotion_scores = data['generated_response'].apply(\n",
    "        lambda x: detect_emotions(x))\n",
    "    data = pd.concat([data, emotion_scores.apply(pd.Series)], axis=1)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Apply the extended evaluation function\n",
    "evaluated_data = pd.read_csv(\n",
    "    '/Users/dipendrapant/Library/CloudStorage/OneDrive-NTNU/ForFun/npj_digital_medicine/TechnicalEvaluationGPT4/data/result/v3/generated_responses.csv')\n",
    "evaluated_data_with_sentiment = evaluate_responses_with_sentiment(\n",
    "    evaluated_data)\n",
    "\n",
    "# Display some sentiment and emotion analysis results\n",
    "print(evaluated_data_with_sentiment[[\n",
    "      'generated_response', 'sentiment', 'pos', 'neu', 'neg', 'compound']].head())\n",
    "\n",
    "############ 7. Summarize Sentiment and Emotion Scores ############\n",
    "\n",
    "# Calculate overall sentiment score\n",
    "overall_sentiment = evaluated_data_with_sentiment['sentiment'].mean()\n",
    "print(f\"Overall Sentiment Score: {overall_sentiment}\")\n",
    "\n",
    "# Calculate average emotion scores\n",
    "average_emotions = evaluated_data_with_sentiment[[\n",
    "    'pos', 'neu', 'neg', 'compound']].mean()\n",
    "print(f\"Average Emotion Scores:\\n{average_emotions}\")\n",
    "\n",
    "# Plotting the emotion distributions\n",
    "plt.figure(figsize=(10, 6))\n",
    "average_emotions.plot(kind='bar', color=[\n",
    "                      'green', 'blue', 'red', 'skyblue'], edgecolor='black', alpha=0.5)\n",
    "#plt.title('Average Emotion Scores')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Emotion')\n",
    "plt.xticks(rotation=0)\n",
    "plt.savefig('/Users/dipendrapant/Library/CloudStorage/OneDrive-NTNU/ForFun/npj_digital_medicine/TechnicalEvaluationGPT4/data/result/v3/gpt_4_turbo_emotion_scores.png', dpi=600)\n",
    "plt.show()\n",
    "\n",
    "############ 8. Plot Histogram of Sentiment Scores ############\n",
    "\n",
    "# Plot the histogram of sentiment scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(evaluated_data_with_sentiment['sentiment'],\n",
    "         bins=50, color='skyblue', edgecolor='black', alpha=0.5)\n",
    "#plt.title('Distribution of Sentiment Scores')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig('/Users/dipendrapant/Library/CloudStorage/OneDrive-NTNU/ForFun/npj_digital_medicine/TechnicalEvaluationGPT4/data/result/v3/gpt_4_turbo_sentiment_score_distribution.png', dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparision of emotion scores with reference text**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ 6. Sentiment Analysis and Emotion Detection ############\n",
    "\n",
    "# Function to perform sentiment analysis\n",
    "def analyze_sentiment(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "# Function to perform emotion detection\n",
    "def detect_emotions(text):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    return sid.polarity_scores(text)\n",
    "\n",
    "# Adding sentiment analysis and emotion detection to the evaluation\n",
    "def evaluate_responses_with_sentiment(data):\n",
    "    # Perform sentiment analysis on both columns\n",
    "    data['generated_sentiment'] = data['generated_response'].apply(analyze_sentiment)\n",
    "    data['response_sentiment'] = data['response'].apply(analyze_sentiment)\n",
    "\n",
    "    # Perform emotion detection on both columns\n",
    "    generated_emotion_scores = data['generated_response'].apply(detect_emotions).apply(pd.Series)\n",
    "    response_emotion_scores = data['response'].apply(detect_emotions).apply(pd.Series)\n",
    "\n",
    "    # Concatenate emotion scores to the original data with prefixes\n",
    "    data = pd.concat([data, generated_emotion_scores.add_prefix('generated_'), response_emotion_scores.add_prefix('response_')], axis=1)\n",
    "    return data\n",
    "\n",
    "# Apply the extended evaluation function\n",
    "evaluated_data = pd.read_csv('/Users/dipendrapant/Library/CloudStorage/OneDrive-NTNU/ForFun/npj_digital_medicine/TechnicalEvaluationGPT4/data/result/v3/generated_responses.csv')\n",
    "evaluated_data_with_sentiment = evaluate_responses_with_sentiment(evaluated_data)\n",
    "\n",
    "# Display some sentiment and emotion analysis results\n",
    "print(evaluated_data_with_sentiment.head())\n",
    "\n",
    "############ 7. Plot Histogram of Sentiment Scores ############\n",
    "\n",
    "# Plot the histogram of sentiment scores for generated responses\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(evaluated_data_with_sentiment['generated_sentiment'],\n",
    "         bins=50, color='skyblue', edgecolor='black', alpha=0.5)\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Sentiment Scores for Generated Responses')\n",
    "plt.show()\n",
    "\n",
    "############ 8. Plot Grouped Emotion Scores ############\n",
    "\n",
    "# Calculate average emotion scores\n",
    "average_generated_emotions = evaluated_data_with_sentiment[['generated_pos', 'generated_neu', 'generated_neg', 'generated_compound']].mean()\n",
    "average_response_emotions = evaluated_data_with_sentiment[['response_pos', 'response_neu', 'response_neg', 'response_compound']].mean()\n",
    "\n",
    "# Creating indices for grouped bar chart\n",
    "index = np.arange(len(average_generated_emotions))\n",
    "bar_width = 0.35\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(index, average_generated_emotions, bar_width, label='Generated', color='orange', alpha=0.8)\n",
    "plt.bar(index + bar_width, average_response_emotions, bar_width, label='Reference', color='blue', alpha=0.8)\n",
    "\n",
    "plt.xlabel('Emotion')\n",
    "plt.ylabel('Scores')\n",
    "#plt.title('Comparison of Emotion Scores between Generated and Response')\n",
    "plt.xticks(index + bar_width / 2, ('Positive', 'Neutral', 'Negative', 'Compound'))\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('/Users/dipendrapant/Library/CloudStorage/OneDrive-NTNU/ForFun/npj_digital_medicine/TechnicalEvaluationGPT4/data/result/v3/gpt_4_turbo_emotion_scores_comparision.eps', dpi=600)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dig_med",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
